<figure class="image">
    <img
        src="https://images.theconversation.com/files/488150/original/file-20221004-12421-klkh40.jpg?ixlib=rb-1.1.0&amp;rect=0%2C206%2C5118%2C2559&amp;q=45&amp;auto=format&amp;w=668&amp;h=324&amp;fit=crop"
        alt="Theoretical Physics – News, Research and Analysis – The Conversation – page 1"
    />
</figure>
<p></p>
<p></p>
<p></p>
<figure class="image">
    <img
        src="https://www.york.ac.uk/media/study/courses/undergraduate/physics/hero-theoretical-yii-mphys-1160.jpg"
        alt="Theoretical Physics (with a year in industry) (MPhys) - Undergraduate, University of York"
    />
</figure>
<p></p>
<p></p>
<p>
    <span class="text-huge"><strong>I</strong></span
    >N THESE PAGES I want to discuss the possibility that the goal of
    theoretical physics might be achieved in the not-too-distant future: say, by
    the end of the century. By this I mean that we might have a complete,
    consistent, and unified theory of the physical interactions that would
    describe all possible observations. Of course, one has to be very cautious
    about making such predictions. We have thought that we were on the brink of
    the final synthesis at least twice before. At the beginning of the century
    it was believed that everything could be understood in terms of continuum
    mechanics. All that was needed was to measure a certain number of
    coefficients of elasticity, viscosity, conductivity, etc. This hope was
    shattered by the discovery of atomic structure and quantum mechanics. Again,
    in the late 1920s Max Born told a group of scientists visiting Göttingen
    that “physics, as we know it, will be over in six months.” This was shortly
    after the discovery by Paul Dirac, a previous holder of the Lucasian Chair,
    of the Dirac equation, which governs the behavior of the electron. It was
    expected that a similar equation would govern the proton, the only other
    supposedly elementary particle known at that time. However, the discovery of
    the neutron and of nuclear forces disappointed those hopes. We now know in
    fact that neither the proton nor the neutron is elementary but that they are
    made up of smaller particles. Nevertheless, we have made a lot of progress
    in recent years, and as I shall describe, there are some grounds for
    cautious optimism that we may see a complete theory within the lifetime of
    some of those reading these pages. Even if we do achieve a complete unified
    theory, we shall not be able to make detailed predictions in any but the
    simplest situations. For example, we already know the physical laws that
    govern everything that we experience in everyday life. As Dirac pointed out,
    his equation was the basis of “most of physics and all of chemistry.”
    However, we have been able to solve the equation only for the very simplest
    system, the hydrogen atom, consisting of one proton and one electron. For
    more complicated atoms with more electrons, let alone for molecules with
    more than one nucleus, we have to resort to approximations and intuitive
    guesses of doubtful validity. For macroscopic systems consisting of 10
    particles or so, we have to use statistical methods and abandon any pretense
    of solving the equations exactly. Although in principle we know the
    equations that govern the whole of biology, we have not been able to reduce
    the study of human behavior to a branch of applied mathematics. What would
    we mean by a complete and unified theory of physics? Our attempts at
    modeling physical reality normally consist of two parts: 1. A set of local
    laws that are obeyed by the various physical quantities. These are usually
    formulated in terms of differential equations. 2. Sets of boundary
    conditions that tell us the state of some regions of the universe at a
    certain time and what effects propagate into it subsequently from the rest
    of the universe. Many people would claim that the role of science is
    confined to the first of these and that theoretical physics will have
    achieved its goal when we have obtained a complete set of local physical
    laws. They would regard the question of the initial conditions for the
    universe as belonging to the realm of metaphysics or religion. In a way,
    this attitude is similar to that of those who in earlier centuries
    discouraged scientific investigation by saying that all natural phenomena
    were the work of God and should not be inquired into. I think that the
    initial conditions of the universe are as suitable a subject for scientific
    study and theory as are the local physical laws. We shall not have a
    complete theory until we can do more than merely say that “things are as
    they are because they were as they were.” The question of the uniqueness of
    the initial conditions is closely related to that of the arbitrariness of
    the local physical laws: One would not regard a theory as complete if it
    contained a number of adjustable parameters such as masses or coupling
    constants that could be given any values one liked. In fact, it seems that
    neither the initial conditions nor the values of the parameters in the
    theory are arbitrary but that they are somehow chosen or picked out very
    carefully. For example, if the proton-neutron mass difference were not about
    twice the mass of the electron, one would not obtain the couple of hundred
    or so stable nucleides that make up the elements and are the basis of
    chemistry and biology. Similarly, if the gravitational mass of the proton
    were significantly different, one would not have had stars in which these
    nucleides could have been built up, and if the initial expansion of the
    universe had been slightly smaller or slightly greater, the universe would
    either have collapsed before such stars could have evolved or would have
    expanded so rapidly that stars would never have been formed by gravitational
    condensation. Indeed, some people have gone so far as to elevate these
    restrictions on the initial conditions and the parameters to the status of a
    principle, the anthropic principle which can be paraphrased as, “Things are
    as they are because we are.” According to one version of the principle,
    there is a very large number of different, separate universes with different
    values of the physical parameters and different initial conditions. Most of
    these universes will not provide the right conditions for the development of
    the complicated structures needed for intelligent life. Only in a small
    number, with conditions and parameters like our own universe, will it be
    possible for intelligent life to develop and to ask the question, “Why is
    the universe as we observe it?” The answer, of course, is that if it were
    otherwise, there would not be anyone to ask the question. The anthropic
    principle does provide some sort of explanation of many of the remarkable
    numerical relations that are observed between the values of different
    physical parameters. However, it is not completely satisfactory; one cannot
    help feeling that there is some deeper explanation. Also, it cannot account
    for all the regions of the universe. For example, our solar system is
    certainly a prerequisite for our existence, as is an earlier generation of
    nearby stars in which heavy elements could have been formed by nuclear
    synthesis. It might even be that the whole of our galaxy was required. But
    there does not seem any necessity for other galaxies to exist, let alone the
    million million or so of them that we see distributed roughly uniformly
    throughout the observable universe. This largescale homogeneity of the
    universe makes it very difficult to believe that the structure of the
    universe is determined by anything so peripheral as some complicated
    molecular structures on a minor planet orbiting a very average star in the
    outer suburbs of a fairly typical spiral galaxy. If we are not going to
    appeal to the anthropic principle, we need some unifying theory to account
    for the initial conditions of the universe and the values of the various
    physical parameters. However, it is too difficult to think up a complete
    theory of everything all at one go (though this does not seem to stop some
    people; I get two or three unified theories in the mail each week). What we
    do instead is to look for partial theories that will describe situations in
    which certain interactions can be ignored or approximated in a simple
    manner. We first divide the material content of the universe into two parts:
    “matter,” particles such as quarks, electrons, muons, etc., and
    “interactions,” such as gravity, electromagnetism, etc. The matter particles
    are described by fields of one-halfinteger spin and obey the Pauli exclusion
    principle, which prevents more than one particle of a given kind from being
    in the same state. This is the reason we can have solid bodies that do not
    collapse to a point or radiate away to infinity. The matter principles are
    divided into two groups: the hadrons, which are composed of quarks; and the
    leptons, which comprise the remainder. The interactions are divided
    phenomenologically into four categories. In order of strength, they are: the
    strong nuclear forces, which interact only with hadrons; electromagnetism,
    which interacts with charged hadrons and leptons; the weak nuclear forces,
    which interact with all hadrons and leptons; and finally, the weakest by
    far, gravity, which interacts with everything. The interactions are
    represented by integer-spin fields that do not obey the Pauli exclusion
    principle. This means they can have many particles in the same state. In the
    case of electromagnetism and gravity, the interactions are also long-range,
    which means that the fields produced by a large number of matter particles
    can all add up to give a field that can be detected on a macroscopic scale.
    For these reasons, they were the first to have theories developed for them:
    gravity by Newton in the seventeenth century, and electromagnetism by
    Maxwell in the nineteenth century. However, these theories were basically
    incompatible because the Newtonian theory was invariant if the whole system
    was given any uniform velocity, whereas the Maxwell theory defined a
    preferred velocity—the speed of light. In the end, it turned out to be the
    Newtonian theory of gravity that had to be modified to make it compatible
    with the invariance properties of the Maxwell theory. This was achieved by
    Einstein’s general theory of relativity, which was formulated in 1915. The
    general relativity theory of gravity and the Maxwell theory of
    electrodynamics were what are called classical theories; that is, they
    involved quantities that were continuously variable and that could, in
    principle at least, be measured to arbitrary accuracy. However, a problem
    arose when one tried to use such theories to construct a model of the atom.
    It had been discovered that the atom consisted of a small, positively
    charged nucleus surrounded by a cloud of negatively charged electrons. The
    natural assumption was that the electrons were in orbit around the nucleus
    as the earth is in orbit around the sun. But the classical theory predicted
    that the electrons would radiate electromagnetic waves. These waves would
    carry away energy and would cause the electrons to spiral into the nucleus,
    producing the collapse of the atom. This problem was overcome by what is
    undoubtedly the greatest achievement in theoretical physics in this century:
    the discovery of the quantum theory. The basic postulate of this is the
    Heisenberg uncertainty principle, which states that certain pairs of
    quantities, such as the position and momentum of a particle, cannot be
    measured simultaneously with arbitrary accuracy. In the case of the atom,
    this meant that in its lowest energy state the electron could not be at rest
    in the nucleus because, in that case, its position would be exactly defined
    (at the nucleus) and its velocity would also be exactly defined (to be
    zero). Instead, both position and velocity would have to be smeared out with
    some probability distribution around the nucleus. In this state the electron
    could not radiate energy in the form of electromagnetic waves because there
    would be no lower energy state for it to go to. In the 1920s and 1930s
    quantum mechanics was applied with great success to systems such as atoms or
    molecules, which have only a finite number of degrees of freedom.
    Difficulties arose, however, when people tried to apply it to the
    electromagnetic field, which has an infinite number of degrees of freedom,
    roughly speaking two for each point of space-time. One can regard these
    degrees of freedom as oscillators, each with its own position and momentum.
    The oscillators cannot be at rest because then they would have exactly
    defined positions and momenta. Instead, each oscillator must have some
    minimum amount of what are called zero-point fluctuations and a nonzero
    energy. The energies of all the infinite number of degrees of freedom would
    cause the apparent mass and charge of the electron to become infinite. A
    procedure called renormalization was developed to overcome this difficulty
    in the late 1940s. It consisted of the rather arbitrary subtraction of
    certain infinite quantities to leave finite remainders. In the case of
    electrodynamics, it was necessary to make two such infinite subtractions,
    one for the mass and the other for the charge of the electron. This
    renormalization procedure has never been put on a very firm conceptual or
    mathematical basis, but it has worked quite well in practice. Its great
    success was the prediction of a small displacement, the Lamb shift, in some
    lines in the spectrum of atomic hydrogen. However, it is not very
    satisfactory from the point of view of attempts to construct a complete
    theory because it does not make any predictions of the values of the finite
    remainders left after making infinite subtractions. Thus, we would have to
    fall back on the anthropic principle to explain why the electron has the
    mass and charge that it does.
</p>
<p>
    <img
        src="https://d3525k1ryd2155.cloudfront.net/h/832/101/987101832.0.m.jpg"
        alt="Is The End In Sight For Theoretical Physics? An Inaugural Lecture by Stephen Hawking - Paperback - 1st"
    />
</p>
